{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../data/')\n",
    "real_world_data_dir = os.path.join('../Dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train_clip_temp.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(data_dir, 'train_clip_temp.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for model using clip\n",
    "\n",
    "\n",
    "### 1. Create the dataset with precomputed embeddings \n",
    "- **mode 1**:\n",
    "\tDownload the dataset from https://hatefulmemeschallenge.com/ \n",
    "\tUse clip to create the jsonl with the embeddings needed for train, evaluation and test or directly download the dataset. To do so uncomment the cell below and run it.\n",
    "\n",
    "\n",
    "- **mode2 (recommended)**\n",
    "Computing the embeddings may take a long time to avoid that is possible to download them from [here](https://drive.google.com/drive/folders/13kap5FjN6cJ7tsckNaib1d0XC8pwfuZp?usp=sharing) and place train_clip_augmented.jsonl, eval_clip.jsonl, test_clip.jsonl, real_life_clip.jsonl directly into the data folder. This method also guarantees that the result obtained are exactly the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, remove it to proceed: data/train_clip_temp.jsonl\n",
      "file already exists, remove it to proceed: data/eval_clip_temp.jsonl\n",
      "file not found: data/test_seen.jsonl\n"
     ]
    }
   ],
   "source": [
    "from scripts.compute_embeddings_clip import makeClipDataset, makeClipAugmentedDataset\n",
    "from core.augmentation import add_unseen\n",
    "# # install clip package\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# # Use clip to create the jsonl with the embeddings needed for train, evaluation and test\n",
    "# model_name = 'ViT-L/14'\n",
    "# makeClipAugmentedDataset(os.path.join(data_dir, 'train_clip_augmented.jsonl'), os.path.join(data_dir, 'train.jsonl'), model_name)\n",
    "# makeClipDataset(os.path.join(data_dir, 'eval_clip.jsonl'), os.path.join(data_dir, 'dev.jsonl'), model_name)\n",
    "# makeClipDataset(os.path.join(data_dir, 'test_clip_seen.jsonl'), os.path.join(data_dir, 'test_seen.jsonl'), model_name)\n",
    "\n",
    "# # compute the real world dataset embeddings\n",
    "# makeClipDataset(os.path.join(data_dir, 'real_life_data_precomputed.jsonl'), os.path.join(real_world_data_dir, 'real_life_data.jsonl'), model_name)\n",
    "\n",
    "# # make the augmented dataset including the unused unseen classes\n",
    "add_unseen(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the model \n",
    "\n",
    "Run the following cell making sure that DATA_DIR in training_clip_freeze is equal to the one where you saved the files\n",
    "\n",
    "Current implementation is based on the paper https://aclanthology.org/2022.nlp4pi-1.20.pdf implemented at https://github.com/gokulkarthik/hateclipper\n",
    "\n",
    "The following cell run the program with the model that we indicated in the report as 'CLIP ViT-L/14 (U + A + B)' if used the pretrained embeddings from drive the results should be equal to the ones in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Exception ignored in: <function _WeakValueDictionary.__init__.<locals>.KeyedRef.remove at 0x76276e7a47c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap>\", line 91, in remove\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "!python ../training_clip_freeze.py --train_data=train_clip_augmented_all.jsonl --eval_data=eval_clip.jsonl --test_data=test_clip_seen.jsonl --paraphrase=True --augment_image=True --balancing=sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell run the program with the model that we indicated in the report as 'CLIP ViT-L/14 (A + B)' if used the pretrained embeddings from drive the results should be equal to the ones in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed embeddings\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/notebooks/../training_clip_freeze.py\", line 248, in <module>\n",
      "    main(args.model_name, args.augment_image, args.paraphrase, args.precomputed, args.balancing, args.train_data, args.eval_data, args.test_data)\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/notebooks/../training_clip_freeze.py\", line 166, in main\n",
      "    train_dataset = ClipHatefulMemeDatasetPrecomputed(os.path.join(DATA_DIR, train_data), augment_img=(augment_image==\"True\"), paraphrase=(paraphrase==\"True\"))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/core/dataset.py\", line 158, in __init__\n",
      "    self.data = [json.loads(jline) for jline in open(data_file_path, 'r').readlines()]\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 322, in decode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../training_clip_freeze.py --train_data=train_clip_augmented.jsonl --eval_data=eval_clip.jsonl --test_data=test_clip_seen.jsonl --paraphrase=True --augment_image=True --balancing=sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell run the program with the model that we indicated in the report as 'CLIP ViT-L/14 (U)' if used the pretrained embeddings from drive the results should be equal to the ones in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python ../training_clip_freeze.py --train_data=train_clip_augmented_all.jsonl --eval_data=eval_clip.jsonl --test_data=test_clip_seen.jsonl --paraphrase=False --augment_image=False --balancing=none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Run without precomputed embeddings\n",
    "\n",
    "It is also possible to run the model without previously precomputing the embeddings. This is not particularly useful given that we used frozen clip model, and running the code this way would mean computing the embeddings 20 times (each epoch) instead of just one. In any case the following code allow to do that but the computational burden required is quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../training_clip_freeze.py --train_data=train.jsonl --eval_data=dev_seen.jsonl --test_data=test_seen.jsonl --paraphrase=True --augment_image=True --balancing=sampler --precomputed=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
