{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from core.model import PretrainedModel\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train_clip_temp.jsonl'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(data_dir, 'train_clip_temp.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for model using clip (WIP)\n",
    "\n",
    "### 0. Get the data\n",
    "\n",
    "Download the dataset from https://hatefulmemeschallenge.com/ \n",
    "\n",
    "### 1. Create the dataset\n",
    "- Check that the data is in base_dir/data/ otherwise change data_dir in previous cell\n",
    "- Use clip to create the jsonl with the embeddings needed for train, evaluation and test\n",
    "- (opt.) Create the jsonl with the embeddings of additional classes \n",
    "- (opt.) Run kmeans to get n clusters in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, remove it to proceed: data/train_clip_temp.jsonl\n",
      "file already exists, remove it to proceed: data/eval_clip_temp.jsonl\n",
      "file not found: data/test_seen.jsonl\n"
     ]
    }
   ],
   "source": [
    "from scripts.compute_embeddings_clip import makeClipDataset\n",
    "# Use clip to create the jsonl with the embeddings needed for train, evaluation and test\n",
    "\n",
    "makeClipDataset(os.path.join(data_dir, 'train_clip.jsonl'), os.path.join(data_dir, 'train.jsonl'))\n",
    "makeClipDataset(os.path.join(data_dir, 'eval_clip.jsonl'), os.path.join(data_dir, 'dev.jsonl'))\n",
    "makeClipDataset(os.path.join(data_dir, 'test_clip.jsonl'), os.path.join(data_dir, 'test_seen.jsonl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.compute_embeddings_clip import makeClipClassesEmbeddings\n",
    "# (opt.) Create the jsonl with the embeddings of additional classes \n",
    "\n",
    "classes = ['Hate Speech', 'Insult', 'Sarcastic', 'Blasphemy', 'Swearing', 'Threat', 'Sexist', 'Racist', 'Offensive', 'Abusive', 'Profane', 'Homophobic', 'Xenophobic', 'Misogynistic', 'Violent', 'Harassment', 'Cyberbullying', 'Trolling', 'Stalking', 'Doxxing', 'Grooming', 'Neutral', 'Positive', 'Negative']\n",
    "#classes += ['Ass', 'Asshole', 'Homosexual', 'Homophobic', 'Racist', 'Gay', 'Lgbt', 'Jew', 'Jewish', 'Anti-semitic', 'Chink', 'Muslims', 'Muslim', 'Isis', 'Islamophobe', 'homophobe ', 'Bombing', 'Sexyhot', 'Bastard', 'Bitch', 'Fucker', 'Cunt', 'Damn', 'Fuck', 'Goddamn', 'Shit', 'Motherfucker', 'Nigga', 'Nigger', 'Prick', 'Shit', 'shit ass', 'Shitass', 'son of a bitch', 'Whore', 'Thot', 'Slut', 'Faggot', 'Dick', 'Pussy', 'Penis', 'Vagina', 'Negro', 'Coon', 'Bitched', 'Sexist', 'Freaking', 'Cock', 'Sucker', 'Lick', 'Licker', 'Rape', 'Molest', 'Anal', 'Buttrape', 'Coont', 'Cancer', 'Sex', 'Retard', 'Fuckface', 'Dumbass', '5h1t', '5hit', 'A_s_s', 'a2m', 'a55', 'adult', 'amateur', 'anal', 'anal impaler', 'anal leakage', 'anilingus', 'anus', 'ar5e', 'arrse', 'arse', 'arsehole', 'ass', 'ass fuck', 'asses', 'assfucker', 'ass-fucker', 'assfukka', 'asshole', 'asshole', 'assholes', 'assmucus', 'assmunch', 'asswhole', 'autoerotic', 'b!tch', 'b00bs', 'b17ch', 'b1tch', 'ballbag', 'ballsack', 'bang (one\\'s) box', 'bangbros', 'bareback', 'bastard', 'beastial', 'beastiality', 'beef curtain', 'bellend', 'bestial', 'bestiality', 'bi+ch', 'biatch', 'bimbos', 'birdlock', 'bitch', 'bitch tit', 'bitcher', 'bitchers', 'bitches', 'bitchin', 'bitching', 'bloody', 'blow job', 'blow me', 'blow mud', 'blowjob', 'blowjobs', 'blue waffle', 'blumpkin', 'boiolas', 'bollock', 'bollok', 'boner', 'boob', 'boobs', 'booobs', 'boooobs', 'booooobs', 'booooooobs', 'breasts', 'buceta', 'bugger', 'bum', 'bunny fucker', 'bust a load', 'busty', 'butt', 'butt fuck', 'butthole', 'buttmuch', 'buttplug', 'c0ck', 'c0cksucker', 'carpet muncher', 'carpetmuncher', 'cawk', 'chink', 'choade', 'chota bags', 'cipa', 'cl1t', 'clit', 'clit licker', 'clitoris', 'clits', 'clitty litter', 'clusterfuck', 'cnut', 'cock', 'cock pocket', 'cock snot', 'cockface', 'cockhead', 'cockmunch', 'cockmuncher', 'cocks', 'cocksuck ', 'cocksucked ', 'cocksucker', 'cock-sucker', 'cocksucking', 'cocksucks ', 'cocksuka', 'cocksukka', 'cok', 'cokmuncher', 'coksucka', 'coon', 'cop some wood', 'cornhole', 'corp whore', 'cox', 'cum', 'cum chugger', 'cum dumpster', 'cum freak', 'cum guzzler', 'cumdump', 'cummer', 'cumming', 'cums', 'cumshot', 'cunilingus', 'cunillingus', 'cunnilingus', 'cunt', 'cunt hair', 'cuntbag', 'cuntlick ', 'cuntlicker ', 'cuntlicking ', 'cunts', 'cuntsicle', 'cunt-struck', 'cut rope', 'cyalis', 'cyberfuc', 'cyberfuck ', 'cyberfucked ', 'cyberfucker', 'cyberfuckers', 'cyberfucking ', 'd1ck', 'damn', 'dick', 'dick hole', 'dick shy', 'dickhead', 'dildo', 'dildos', 'dink', 'dinks', 'dirsa', 'dirty Sanchez', 'dlck', 'dog-fucker', 'doggie style', 'doggiestyle', 'doggin', 'dogging', 'donkeyribber', 'doosh', 'duche', 'dyke', 'eat a dick', 'eat hair pie', 'ejaculate', 'ejaculated', 'ejaculates ', 'ejaculating ', 'ejaculatings', 'ejaculation', 'ejakulate', 'erotic', 'f u c k', 'f u c k e r', 'f_u_c_k', 'f4nny', 'facial', 'fag', 'fagging', 'faggitt', 'faggot', 'faggs', 'fagot', 'fagots', 'fags', 'fanny', 'fannyflaps', 'fannyfucker', 'fanyy', 'fatass', 'fcuk', 'fcuker', 'fcuking', 'feck', 'fecker', 'felching', 'fellate', 'fellatio', 'fingerfuck ', 'fingerfucked ', 'fingerfucker ', 'fingerfuckers', 'fingerfucking ', 'fingerfucks ', 'fist fuck', 'fistfuck', 'fistfucked ', 'fistfucker ', 'fistfuckers ', 'fistfucking ', 'fistfuckings ', 'fistfucks ', 'flange', 'flog the log', 'fook', 'fooker', 'fuck hole', 'fuck puppet', 'fuck trophy', 'fuck yo mama', 'fuck', 'fucka', 'fuck-ass', 'fuck-bitch', 'fucked', 'fucker', 'fuckers', 'fuckhead', 'fuckheads', 'fuckin', 'fucking', 'fuckings', 'fuckingshitmotherfucker', 'fuckme ', 'fuckmeat', 'fucks', 'fucktoy', 'fuckwhit', 'fuckwit', 'fudge packer', 'fudgepacker', 'fuk', 'fuker', 'fukker', 'fukkin', 'fuks', 'fukwhit', 'fukwit', 'fux', 'fux0r', 'gangbang', 'gangbang', 'gang-bang', 'gangbanged ', 'gangbangs ', 'gassy ass', 'gaylord', 'gaysex', 'goatse', 'god', 'god damn', 'god-dam', 'goddamn', 'goddamned', 'god-damned', 'ham flap', 'hardcoresex ', 'hell', 'heshe', 'hoar', 'hoare', 'hoer', 'homo', 'homoerotic', 'hore', 'horniest', 'horny', 'hotsex', 'how to kill', 'how to murdep', 'jackoff', 'jack-off ', 'jap', 'jerk', 'jerk-off ', 'jism', 'jiz ', 'jizm ', 'jizz', 'kawk', 'kinky Jesus', 'knob', 'knob end', 'knobead', 'knobed', 'knobend', 'knobend', 'knobhead', 'knobjocky', 'knobjokey', 'kock', 'kondum', 'kondums', 'kum', 'kummer', 'kumming', 'kums', 'kunilingus', 'kwif', 'l3i+ch', 'l3itch', 'labia', 'LEN', 'lmao', 'lmfao', 'lmfao', 'lust', 'lusting', 'm0f0', 'm0fo', 'm45terbate', 'ma5terb8', 'ma5terbate', 'mafugly', 'masochist', 'masterb8', 'masterbat*', 'masterbat3', 'masterbate', 'master-bate', 'masterbation', 'masterbations', 'masturbate', 'mof0', 'mofo', 'mo-fo', 'mothafuck', 'mothafucka', 'mothafuckas', 'mothafuckaz', 'mothafucked ', 'mothafucker', 'mothafuckers', 'mothafuckin', 'mothafucking ', 'mothafuckings', 'mothafucks', 'mother fucker', 'mother fucker', 'motherfuck', 'motherfucked', 'motherfucker', 'motherfuckers', 'motherfuckin', 'motherfucking', 'motherfuckings', 'motherfuckka', 'motherfucks', 'muff', 'muff puff', 'mutha', 'muthafecker', 'muthafuckker', 'muther', 'mutherfucker', 'n1gga', 'n1gger', 'nazi', 'need the dick', 'nigg3r', 'nigg4h', 'nigga', 'niggah', 'niggas', 'niggaz', 'nigger', 'niggers ', 'nob', 'nob jokey', 'nobhead', 'nobjocky', 'nobjokey', 'numbnuts', 'nut butter', 'nutsack', 'omg', 'orgasim ', 'orgasims ', 'orgasm', 'orgasms ', 'p0rn', 'pawn', 'pecker', 'penis', 'penisfucker', 'phonesex', 'phuck', 'phuk', 'phuked', 'phuking', 'phukked', 'phukking', 'phuks', 'phuq', 'pigfucker', 'pimpis', 'piss', 'pissed', 'pisser', 'pissers', 'pisses ', 'pissflaps', 'pissin ', 'pissing', 'pissoff ', 'poop', 'porn', 'porno', 'pornography', 'pornos', 'prick', 'pricks ', 'pron', 'pube', 'pusse', 'pussi', 'pussies', 'pussy', 'pussy fart', 'pussy palace', 'pussys ', 'queaf', 'queer', 'rectum', 'retard', 'rimjaw', 'rimming', 's hit', 's.o.b.', 's_h_i_t', 'sadism', 'sadist', 'sandbar', 'sausage queen', 'schlong', 'screwing', 'scroat', 'scrote', 'scrotum', 'semen', 'sex', 'sh!+', 'sh!t', 'sh1t', 'shag', 'shagger', 'shaggin', 'shagging', 'shemale', 'shi+', 'shit', 'shit fucker', 'shitdick', 'shite', 'shited', 'shitey', 'shitfuck', 'shitfull', 'shithead', 'shiting', 'shitings', 'shits', 'shitted', 'shitter', 'shitters ', 'shitting', 'shittings', 'shitty ', 'skank', 'slope', 'slut', 'slut bucket', 'sluts', 'smegma', 'smut', 'snatch', 'son-of-a-bitch', 'spac', 'spunk', 't1tt1e5', 't1tties', 'teets', 'teez', 'testical', 'testicle', 'tit', 'tit wank', 'titfuck', 'tits', 'titt', 'tittie5', 'tittiefucker', 'titties', 'tittyfuck', 'tittywank', 'titwank', 'tosser', 'turd', 'tw4t', 'twat', 'twathead', 'twatty', 'twunt', 'twunter', 'v14gra', 'v1gra', 'vagina', 'viagra', 'vulva', 'w00se', 'wang', 'wank', 'wanker', 'wanky', 'whoar', 'whore', 'willies', 'willy', 'wtf', 'xrated', 'xxx', 'sucker', 'dumbass', 'Kys', 'Kill', 'Die', 'Cliff', 'Bridge', 'Shooting', 'Shoot', 'Bomb', 'Terrorist', 'Terrorism', 'Bombed', 'Trump', 'Maga', 'Conservative', 'Make america great again', 'Far right', 'Necrophilia', 'Mongoloid', 'Furfag', 'Cp', 'Pedo', 'Pedophile', 'Pedophilia', 'Child predator', 'Predatory', 'Depression', 'Cut myself', 'I want to die', 'Fuck life', 'Redtube', 'Loli', 'Lolicon', 'Cub']\n",
    "\n",
    "classes_embeddings = makeClipClassesEmbeddings(os.path.join(data_dir, 'classes_clip.jsonl'), classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (opt.) Run kmeans to get n clusters in the embedding space\n",
    "X = np.array([el['embedding'][0].cpu().detach().numpy().tolist() for el in classes_embeddings])\n",
    "print(X.shape)\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "reduced_data = pca.transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=64, random_state=0, n_init=\"auto\").fit(reduced_data)\n",
    "centers = pca.inverse_transform(kmeans.cluster_centers_)\n",
    "\n",
    "with open(os.path.join(data_dir,'classes_clip_centers.jsonl'), 'w') as f:\n",
    "  for c in tqdm(centers, total=len(centers)):\n",
    "    text = c\n",
    "    f.write(json.dumps({'embedding': text.tolist()}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the model \n",
    "\n",
    "Run the following cell making sure that DATA_DIR in training_clip_freeze is equal to the one where you saved the files\n",
    "\n",
    "Current implementation is based on the paper https://aclanthology.org/2022.nlp4pi-1.20.pdf implemented at https://github.com/gokulkarthik/hateclipper\n",
    "\n",
    "However the results are not the one described in it even though the architecture should be similar. Obtained AUC-ROC is around 0.70% while the one described in the paper is around 84%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1028]         790,532\n",
      "            Linear-2              [-1, 1, 1028]         790,532\n",
      "           Dropout-3              [-1, 1, 1028]               0\n",
      "           Dropout-4              [-1, 1, 1028]               0\n",
      "            Linear-5              [-1, 1, 1028]       1,057,812\n",
      "              ReLU-6              [-1, 1, 1028]               0\n",
      "              ReLU-7              [-1, 1, 1028]               0\n",
      "              ReLU-8              [-1, 1, 1028]               0\n",
      "              ReLU-9              [-1, 1, 1028]               0\n",
      "          Dropout-10              [-1, 1, 1028]               0\n",
      "          Dropout-11              [-1, 1, 1028]               0\n",
      "          Dropout-12              [-1, 1, 1028]               0\n",
      "          Dropout-13              [-1, 1, 1028]               0\n",
      "           Linear-14                 [-1, 1, 2]           2,058\n",
      "          Dropout-15              [-1, 1, 1028]               0\n",
      "          Dropout-16              [-1, 1, 1028]               0\n",
      "          Dropout-17              [-1, 1, 1028]               0\n",
      "           Linear-18              [-1, 1, 1028]       1,057,812\n",
      "             ReLU-19              [-1, 1, 1028]               0\n",
      "             ReLU-20              [-1, 1, 1028]               0\n",
      "             ReLU-21              [-1, 1, 1028]               0\n",
      "             ReLU-22              [-1, 1, 1028]               0\n",
      "          Dropout-23              [-1, 1, 1028]               0\n",
      "          Dropout-24              [-1, 1, 1028]               0\n",
      "          Dropout-25              [-1, 1, 1028]               0\n",
      "          Dropout-26              [-1, 1, 1028]               0\n",
      "           Linear-27                 [-1, 1, 2]           2,058\n",
      "          Dropout-28              [-1, 1, 1028]               0\n",
      "          Dropout-29              [-1, 1, 1028]               0\n",
      "          Dropout-30              [-1, 1, 1028]               0\n",
      "           Linear-31              [-1, 1, 1028]       1,057,812\n",
      "             ReLU-32              [-1, 1, 1028]               0\n",
      "             ReLU-33              [-1, 1, 1028]               0\n",
      "             ReLU-34              [-1, 1, 1028]               0\n",
      "             ReLU-35              [-1, 1, 1028]               0\n",
      "          Dropout-36              [-1, 1, 1028]               0\n",
      "          Dropout-37              [-1, 1, 1028]               0\n",
      "          Dropout-38              [-1, 1, 1028]               0\n",
      "          Dropout-39              [-1, 1, 1028]               0\n",
      "           Linear-40                 [-1, 1, 2]           2,058\n",
      "================================================================\n",
      "Total params: 4,760,674\n",
      "Trainable params: 4,760,674\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.25\n",
      "Forward/backward pass size (MB): 0.29\n",
      "Params size (MB): 18.16\n",
      "Estimated Total Size (MB): 20.70\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "100%|█████████████████████████████████████████| 133/133 [00:12<00:00, 10.24it/s]\n",
      "Epoch 1 average loss: 1.976\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 33.06it/s]\n",
      "Train loss: 1.839, macro f1: 0.741, micro f1: 0.750, accuracy: 0.750, precision: 0.620, recall: 0.784, ROC-AUC: 0.757\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 34.84it/s]\n",
      "Evaluation loss: 2.009, macro f1: 0.656, micro f1: 0.656, accuracy: 0.656, precision: 0.668, recall: 0.620, ROC-AUC: 0.656\n",
      "100%|█████████████████████████████████████████| 133/133 [00:18<00:00,  7.07it/s]\n",
      "Epoch 2 average loss: 1.840\n",
      "100%|█████████████████████████████████████████| 133/133 [00:06<00:00, 20.31it/s]\n",
      "Train loss: 1.758, macro f1: 0.792, micro f1: 0.800, accuracy: 0.800, precision: 0.680, recall: 0.839, ROC-AUC: 0.809\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 20.87it/s]\n",
      "Evaluation loss: 1.999, macro f1: 0.680, micro f1: 0.682, accuracy: 0.682, precision: 0.716, recall: 0.604, ROC-AUC: 0.682\n",
      "100%|█████████████████████████████████████████| 133/133 [00:16<00:00,  8.26it/s]\n",
      "Epoch 3 average loss: 1.764\n",
      "100%|█████████████████████████████████████████| 133/133 [00:03<00:00, 37.14it/s]\n",
      "Train loss: 1.683, macro f1: 0.834, micro f1: 0.841, accuracy: 0.841, precision: 0.727, recall: 0.893, ROC-AUC: 0.853\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 43.13it/s]\n",
      "Evaluation loss: 2.003, macro f1: 0.685, micro f1: 0.688, accuracy: 0.688, precision: 0.735, recall: 0.588, ROC-AUC: 0.688\n",
      "100%|█████████████████████████████████████████| 133/133 [00:14<00:00,  8.99it/s]\n",
      "Epoch 4 average loss: 1.685\n",
      "100%|█████████████████████████████████████████| 133/133 [00:05<00:00, 25.45it/s]\n",
      "Train loss: 1.603, macro f1: 0.844, micro f1: 0.849, accuracy: 0.849, precision: 0.723, recall: 0.939, ROC-AUC: 0.869\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 25.46it/s]\n",
      "Evaluation loss: 1.940, macro f1: 0.676, micro f1: 0.676, accuracy: 0.676, precision: 0.683, recall: 0.656, ROC-AUC: 0.676\n",
      "100%|█████████████████████████████████████████| 133/133 [00:15<00:00,  8.47it/s]\n",
      "Epoch 5 average loss: 1.622\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 32.99it/s]\n",
      "Train loss: 1.571, macro f1: 0.869, micro f1: 0.874, accuracy: 0.874, precision: 0.764, recall: 0.941, ROC-AUC: 0.889\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 30.33it/s]\n",
      "Evaluation loss: 2.009, macro f1: 0.685, micro f1: 0.686, accuracy: 0.686, precision: 0.705, recall: 0.640, ROC-AUC: 0.686\n",
      "100%|█████████████████████████████████████████| 133/133 [00:17<00:00,  7.67it/s]\n",
      "Epoch 6 average loss: 1.580\n",
      "100%|█████████████████████████████████████████| 133/133 [00:03<00:00, 36.97it/s]\n",
      "Train loss: 1.535, macro f1: 0.860, micro f1: 0.865, accuracy: 0.865, precision: 0.738, recall: 0.965, ROC-AUC: 0.887\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 44.82it/s]\n",
      "Evaluation loss: 2.011, macro f1: 0.686, micro f1: 0.686, accuracy: 0.686, precision: 0.691, recall: 0.672, ROC-AUC: 0.686\n",
      "100%|█████████████████████████████████████████| 133/133 [00:13<00:00,  9.94it/s]\n",
      "Epoch 7 average loss: 1.557\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 31.22it/s]\n",
      "Train loss: 1.513, macro f1: 0.894, micro f1: 0.899, accuracy: 0.899, precision: 0.802, recall: 0.952, ROC-AUC: 0.910\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 38.38it/s]\n",
      "Evaluation loss: 2.047, macro f1: 0.694, micro f1: 0.694, accuracy: 0.694, precision: 0.708, recall: 0.660, ROC-AUC: 0.694\n",
      "100%|█████████████████████████████████████████| 133/133 [00:13<00:00,  9.62it/s]\n",
      "Epoch 8 average loss: 1.536\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 28.30it/s]\n",
      "Train loss: 1.488, macro f1: 0.908, micro f1: 0.912, accuracy: 0.912, precision: 0.826, recall: 0.956, ROC-AUC: 0.922\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 31.42it/s]\n",
      "Evaluation loss: 2.095, macro f1: 0.680, micro f1: 0.682, accuracy: 0.682, precision: 0.718, recall: 0.600, ROC-AUC: 0.682\n",
      "100%|█████████████████████████████████████████| 133/133 [00:15<00:00,  8.56it/s]\n",
      "Epoch 9 average loss: 1.516\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 30.93it/s]\n",
      "Train loss: 1.483, macro f1: 0.926, micro f1: 0.930, accuracy: 0.930, precision: 0.859, recall: 0.962, ROC-AUC: 0.937\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 40.43it/s]\n",
      "Evaluation loss: 2.140, macro f1: 0.684, micro f1: 0.686, accuracy: 0.686, precision: 0.720, recall: 0.608, ROC-AUC: 0.686\n",
      "100%|█████████████████████████████████████████| 133/133 [00:16<00:00,  8.13it/s]\n",
      "Epoch 10 average loss: 1.500\n",
      "100%|█████████████████████████████████████████| 133/133 [00:05<00:00, 24.38it/s]\n",
      "Train loss: 1.456, macro f1: 0.917, micro f1: 0.921, accuracy: 0.921, precision: 0.832, recall: 0.977, ROC-AUC: 0.933\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 23.69it/s]\n",
      "Evaluation loss: 2.082, macro f1: 0.671, micro f1: 0.672, accuracy: 0.672, precision: 0.690, recall: 0.624, ROC-AUC: 0.672\n",
      "100%|█████████████████████████████████████████| 133/133 [00:16<00:00,  7.83it/s]\n",
      "Epoch 11 average loss: 1.484\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 28.08it/s]\n",
      "Train loss: 1.440, macro f1: 0.938, micro f1: 0.942, accuracy: 0.942, precision: 0.884, recall: 0.964, ROC-AUC: 0.946\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 36.81it/s]\n",
      "Evaluation loss: 2.158, macro f1: 0.664, micro f1: 0.666, accuracy: 0.666, precision: 0.699, recall: 0.584, ROC-AUC: 0.666\n",
      "100%|█████████████████████████████████████████| 133/133 [00:15<00:00,  8.45it/s]\n",
      "Epoch 12 average loss: 1.466\n",
      "100%|█████████████████████████████████████████| 133/133 [00:04<00:00, 32.42it/s]\n",
      "Train loss: 1.427, macro f1: 0.940, micro f1: 0.944, accuracy: 0.944, precision: 0.879, recall: 0.978, ROC-AUC: 0.951\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 36.71it/s]\n",
      "Evaluation loss: 2.145, macro f1: 0.664, micro f1: 0.666, accuracy: 0.666, precision: 0.697, recall: 0.588, ROC-AUC: 0.666\n",
      " 53%|██████████████████████▍                   | 71/133 [00:07<00:06,  9.47it/s]^C\n",
      " 53%|██████████████████████▍                   | 71/133 [00:07<00:06,  9.04it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/training_clip_freeze.py\", line 49, in <module>\n",
      "    main()\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/training_clip_freeze.py\", line 42, in main\n",
      "    train_freeze(\n",
      "  File \"/home/ceru/Documents/2o_semestre_23-24/deep_learning/hate-speach-detection/core/loop.py\", line 90, in train_freeze\n",
      "    optimizer.step()\n",
      "  File \"/home/ceru/.virtualenvs/ml/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ceru/.virtualenvs/ml/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ceru/.virtualenvs/ml/lib/python3.11/site-packages/torch/optim/adamw.py\", line 187, in step\n",
      "    adamw(\n",
      "  File \"/home/ceru/.virtualenvs/ml/lib/python3.11/site-packages/torch/optim/adamw.py\", line 339, in adamw\n",
      "    func(\n",
      "  File \"/home/ceru/.virtualenvs/ml/lib/python3.11/site-packages/torch/optim/adamw.py\", line 415, in _single_tensor_adamw\n",
      "    param.mul_(1 - lr * weight_decay)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd ./..\n",
    "!python training_clip_freeze.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
